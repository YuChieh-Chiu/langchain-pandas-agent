{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Langchain Pandas Agent with Llama3.1 8B**\n\n**reference:**\n- [run streamlit in kaggle with ngrok](https://www.kaggle.com/code/amlanmohanty1/build-web-app-for-heart-disease-with-streamlit#Write-a-file-for-creating-web-app)\n- [langchain agent with gemma:2b](https://www.youtube.com/watch?v=u3SGDvOVyO4)\n- [work with ollama](https://stackoverflow.com/questions/78394289/running-ollama-on-kaggle)\n- [ollama official website](https://ollama.com/library/llama3.1:8b)","metadata":{}},{"cell_type":"markdown","source":"## **1. pip install requirements**","metadata":{}},{"cell_type":"code","source":"pip install langchain langchain-experimental langchain-ollama openpyxl tabulate streamlit pyngrok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyngrok import conf, ngrok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl https://ollama.ai/install.sh | sh\nimport subprocess\nprocess = subprocess.Popen(\"ollama serve\", shell=True) # runs on a different thread\n!pip install ollama\nimport ollama","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download model\n!ollama pull llama3.1:8b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. write a file for creating web app**\n### **2-1. import packages**\n### **2-2. define some functions we need**\n### **2-3. build streamlit web app**\n\n**Security Notice of create_pandas_dataframe_agent:**\n```\nThis agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Failure to run this code in a properly sandboxed environment can lead to arbitrary code execution vulnerabilities, which can lead to data breaches, data loss, or other security incidents.\n\nDo not use this code with untrusted inputs, with elevated permissions, or without consulting your security team about proper sandboxing!\n\nYou must opt-in to use this functionality by setting allow_dangerous_code=True.\n```","metadata":{}},{"cell_type":"code","source":"%%writefile pandas_agent_app.py\n\n# import packages required\nimport pandas as pd\nimport streamlit as st\nfrom langchain.agents import AgentType\nfrom langchain_experimental.agents import create_pandas_dataframe_agent\nfrom langchain_ollama import ChatOllama\n\n# streamlit web app configuration\nst.set_page_config(\n    page_title=\"Chat with CSV/XLSX using Llama3.1\",\n    page_icon=\"ü¶ô\",\n    layout=\"centered\"\n)\n\n# read csv, xlsx files uploaded to streamlit web app\ndef read_tabular_data(file):\n    if file.name.endswith(\".csv\"):\n        return pd.read_csv(file)\n    elif file.name.endswith(\".xlsx\"):\n        return pd.read_excel(file)\n    else:\n        raise ValueError(\"your file doesn't end with `.csv` or `.xlsx`, please check it.\")\n\n# streamlit page title\nst.title(\"üíª Tabular Data Chat ft. Langchain Agent, Llama3.1, Ollama\")\n\n# initialize chat history\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\n\n# initiate tabular_data in session state\nif \"tabular_data\" not in st.session_state:\n    st.session_state.tabular_data = None\n\n# file upload widget\nuploaded_file = st.file_uploader(\"please upload your `.csv` or `.xlsx` file\", \n                                 type=[\"csv\", \"xlsx\"])\nif uploaded_file:\n    st.session_state.tabular_data = read_tabular_data(uploaded_file)\n    st.write(\"üìΩÔ∏è Tabular Data PreviewÔºö\")\n    st.dataframe(st.session_state.tabular_data.head())\n\n# display chat history\nfor message in st.session_state.chat_history:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# load llm (Llama3.1) using Ollama\nllm = ChatOllama(model=\"llama3.1:8b\", temperature=0.1)\n\n# create a llm agent\ntabular_agent = create_pandas_dataframe_agent(\n    llm,\n    st.session_state.tabular_data,\n    agent_type=\"tool-calling\",\n    verbose=True,\n    allow_dangerous_code=True,\n    agent_executor_kwargs={\"handle_parsing_errors\": True}\n)\n\n# define system prompt\nsystem_prompt = \"you are a helpful assistant. the user you are helping speaks Traditional Chinese. please answer in Traditional Chinese as well.\"\n\n# user question\nuser_query = st.chat_input(\"ask something in your tabular data...\")\nif user_query:\n    # display user question\n    st.chat_message(\"user\").markdown(user_query)\n    # save user question into chat history\n    st.session_state.chat_history.append({\n        \"role\":\"user\",\n        \"content\": user_query\n    })\n    # give the system prompt and chat history including the latest user question to llm agent\n    messages = [\n        {\n            \"role\":\"system\", \n            \"content\": system_prompt\n        },\n        *st.session_state.chat_history\n    ]\n    # call llm agent to response the user question\n    response = tabular_agent.invoke(messages)\n    llm_reply = response[\"output\"]\n    # save llm response into chat history\n    st.session_state.chat_history.append({\n        \"role\":\"assistant\", \n        \"content\": llm_reply\n    })\n    # display llm response\n    with st.chat_message(\"assistant\"):\n        st.markdown(llm_reply)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. run streamlit web app with ngrok**\n**if you want to run the app even after the terminal is closed, do the following**\n\n**code:**<br>\n```!nohup streamlit run app.py```\n\n**explained by gpt:**<br>\nThe command `!nohup streamlit run app.py` means the following:\n\n- `!`: In some Python environments (such as Jupyter Notebook or Kaggle Notebook), `!` is used to execute system commands.\n- `nohup`: This command stands for \"no hang up\". It allows a command to continue running even after the terminal is closed. By default, it writes the command's output to a file called `nohup.out` unless you specify another file.\n- `streamlit run app.py`: This part of the command starts a Streamlit application and runs the `app.py` file. Streamlit is an open-source framework used for building data applications.\n\nPutting it all together, the command means:\n\n- In a Python environment that allows system commands (like Jupyter Notebook),\n- Using the `nohup` command to ensure that the Streamlit application continues to run even if the terminal is closed,\n- And running the `app.py` Streamlit application.\n\nThis ensures that your Streamlit application runs in the background and continues to operate even if you close the current command line or notebook environment.","metadata":{}},{"cell_type":"code","source":"ngrok_token = input(\"copy and paste your ngrok token here:\") # keep it secret\n\nif ngrok_token:\n    conf.get_default().auth_token = ngrok_token\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(8501)\n        print(\"url:\", ssh_tunnel.public_url)\n    else:\n        print(\"url:\", ssh_tunnels[0].public_url)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nget_ipython().system = os.system","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!streamlit run ./pandas_agent_app.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}